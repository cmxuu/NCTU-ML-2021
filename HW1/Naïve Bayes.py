# -*- coding: utf-8 -*-
"""ML_HW1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CwgjTJllCG19MlWOWGxzODKVJ--u3uK5

# **ML1 - Naïve Bayes**
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import norm
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import CategoricalNB, GaussianNB
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, classification_report

"""## **MUSHROOM**

### **Data Input**
"""

MR_feature=['label', 'cap-shape', 'cap-surface', 'cap-color', 'bruises?', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat']
MR_DF=pd.read_csv("./agaricus-lepiota.data", sep=',', names=MR_feature)
MR_DF

"""### **Data Visualization**"""

# https://www.jianshu.com/p/0c3e2e3f1d14（histplot）

for i in MR_feature[1:23]:

  # value frequency
  sns.histplot(data=MR_DF,x=MR_DF[i])
  plt.show()

  # labels
  sns.histplot(data=MR_DF,x=MR_DF[i],hue="label",multiple="dodge")
  plt.show()

"""### **Data Preprocessing**"""

# Drop features 處理缺失值
# 先把?改成空值，再用dropna把那行去掉
MR_DF=MR_DF.replace('?', pd.NaT)    
MR_DF=MR_DF.dropna(axis=1)
MR_DF.describe()

# Shuffle Data 弄亂順序
MR_shuffled=MR_DF.sample(frac=1).reset_index(drop=True)
MR_shuffled

# Text -> Number
LE=LabelEncoder()
for i in MR_shuffled.columns:
  MR_shuffled[i]=LE.fit_transform(MR_shuffled[i])
MR_shuffled

"""### **Model Construction**"""

# https://blog.csdn.net/Hanx09/article/details/104641221（Holdout、K-fold）
# https://blog.csdn.net/W_weiying/article/details/81411257（loc、iloc）

def MR_Holdout(MR,laplace):
  x=MR.drop('label',axis=1)
  y=MR['label']
  x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.7)
  if laplace==True:
    train=CategoricalNB()
  else:
    train=CategoricalNB(alpha=1e-10) # 1e-10->0
  train.fit(x_train,y_train)
  y_predict=train.predict(x_test)
  return y_test,y_predict

def MR_Kfold(MR,laplace):
  x=MR.drop('label',axis=1)
  y=MR['label']
  y_TEST=[]
  y_PREDICT=[]
  kf=KFold(n_splits=3,shuffle=True)
  for index_train,index_test in kf.split(MR):
    x_train=x.iloc[index_train]
    x_test=x.iloc[index_test]
    y_train=y[index_train]
    y_test=y[index_test]
    if laplace==True:
      train=CategoricalNB()
    else:
      train=CategoricalNB(alpha=1e-10)
    train.fit(x_train,y_train)
    y_predict=train.predict(x_test)
    y_TEST.extend(y_test)
    y_PREDICT.extend(y_predict)
  return y_TEST,y_PREDICT


# http://www.taroballz.com/2018/07/09/ML_Classifier_Model/
# //https://clay-atlas.com/blog/2020/01/13/machine-learning-chinese-tutorial-scikit-learn-precision-recall-f1/
# //https://www.cnblogs.com/178mz/p/8558435.html
# //https://blog.csdn.net/weixin_44436677/article/details/105985358

def MR_Results(y_test,y_predict):
  print("Confusion matrix: \n",confusion_matrix(y_test,y_predict))
  print("\nReport: \n",classification_report(y_test,y_predict,target_names=["edible","poisonous"]))

"""### **Train-Test-Split + Results**

**Holdout** with laplace smoothing
"""

MR_Holdout_with_test,MR_Holdout_with_predict=MR_Holdout(MR_shuffled,laplace=True)
MR_Results(MR_Holdout_with_test,MR_Holdout_with_predict)

"""**Holdout** without laplace smoothing"""

MR_Holdout_without_test,MR_Holdout_without_predict=MR_Holdout(MR_shuffled,laplace=False)
MR_Results(MR_Holdout_without_test,MR_Holdout_without_predict)

"""**K-fold** with laplace smoothing"""

MR_Kfold_with_test,MR_Kfold_with_predict=MR_Kfold(MR_shuffled,laplace=True)
MR_Results(MR_Kfold_with_test,MR_Kfold_with_predict)

"""**K-fold** without laplace smoothing"""

MR_Kfold_without_test,MR_Kfold_without_predict=MR_Kfold(MR_shuffled,laplace=False)
MR_Results(MR_Kfold_without_test,MR_Kfold_without_predict)

"""### **Questions**"""

MR_list=['n','b','c','g','o','p','e','w','y']
MR_Qx=MR_DF[MR_DF['label']=='e']
MR_Qx=MR_Qx.to_numpy()
MR_Qy=MR_DF.iloc[:,[0]]
MR_Qy=MR_Qy[MR_Qy['label']=='e']
MR_Qy=MR_Qy.to_numpy()
MR_Qp_without=[]
MR_Qp_with=[]

for i in MR_list:
  y=np.sum(MR_Qy=='e')
  xy=np.sum(MR_Qx[:,14]==i)
  t=np.unique(MR_Qx[:,14]).shape[0]
  MR_Qp_without.append(xy/float(y))
  MR_Qp_with.append((xy+1)/float(y+1*t))

print("Without Laplace smoothing:")
for i in range(0,9):
  print("%s %.6f"%(MR_list[i],MR_Qp_without[i]))
plt.bar(x=MR_list,height=MR_Qp_without)
plt.show()
print("\nWith Laplace smoothing:",)
for i in range(0,9):
  print("%s %.6f"%(MR_list[i],MR_Qp_with[i]))
plt.bar(x=MR_list,height=MR_Qp_with)
plt.show()

"""## **IRIS**

### **Data Input**
"""

IRIS_feature=['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']
IRIS_DF=pd.read_csv("./iris.data", sep=',', names=IRIS_feature)
IRIS_DF

"""### **Data Visualization**"""

print("Average:")
for i in IRIS_feature[0:4]:
  print("%s\t%.6f"%(i,IRIS_DF[i].mean()))
print("\n")

print("Standard deviation:")
for i in IRIS_feature[0:4]:
  print("%s\t%.6f"%(i,IRIS_DF[i].std()))
print("\n")

for i in IRIS_feature[0:4]:
  sns.histplot(data=IRIS_DF,x=IRIS_DF[i],bins=10)
  plt.show()
  sns.histplot(data=IRIS_DF,x=IRIS_DF[i],bins=10,hue="class", multiple="dodge")
  plt.show()

"""### **Data Preprocessing**"""

IRIS_DF=IRIS_DF.replace('?', pd.NaT)    
IRIS_DF=IRIS_DF.dropna(axis=1)
IRIS_DF.describe()

IRIS_shuffled=IRIS_DF.sample(frac=1).reset_index(drop=True)
IRIS_shuffled

"""### **Model Construction**"""

def IRIS_Holdout(IRIS):
  x=IRIS.drop('class',axis=1)
  y=IRIS['class']
  x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.7)
  train=GaussianNB()
  train.fit(x_train,y_train)
  y_predict=train.predict(x_test)
  return y_test,y_predict

def IRIS_Kfold(IRIS):
  x=IRIS.drop('class',axis=1)
  y=IRIS['class']
  y_TEST=[]
  y_PREDICT=[]
  kf=KFold(n_splits=3,shuffle=True)
  for index_train,index_test in kf.split(IRIS):
    x_train=x.iloc[index_train]
    x_test=x.iloc[index_test]
    y_train=y[index_train]
    y_test=y[index_test]
    train=GaussianNB()
    train.fit(x_train,y_train)
    y_predict=train.predict(x_test)
    y_TEST.extend(y_test)
    y_PREDICT.extend(y_predict)
  return y_TEST,y_PREDICT

def IRIS_Results(y_test,y_predict,Vtype):
  if Vtype=='H':
    print("Confusion matrix: \n",confusion_matrix(y_test,y_predict))
  else:
    print("Confusion matrix: \n",confusion_matrix(y_test,y_predict)/3)
  print("\nReport: \n",classification_report(y_test,y_predict))

"""### **Train-Test-Split + Results**

**Holdout**
"""

IRIS_Holdout_test,IRIS_Holdout_predict=IRIS_Holdout(IRIS_shuffled)
IRIS_Results(IRIS_Holdout_test,IRIS_Holdout_predict,'H')

"""**K-fold**"""

IRIS_Kfold_test,IRIS_Kfold_predict=IRIS_Kfold(IRIS_shuffled)
IRIS_Results(IRIS_Kfold_test,IRIS_Kfold_predict,'K')

"""### **Questions**"""

# https://seaborn.pydata.org/tutorial/distributions.html
# https://stackoverflow.com/questions/10138085/how-to-plot-normal-distribution

IRIS_mean=IRIS_DF[IRIS_DF['class']=='Iris-versicolor'].mean(axis=0).get('petal-length')
IRIS_std=IRIS_DF[IRIS_DF['class']=='Iris-versicolor'].var(axis=0).get('petal-length')**0.5
print("𝜇: %.6f"%IRIS_mean)
print("𝜎: %.6f\n"%IRIS_std)

IRIS_axis=np.arange(IRIS_mean-5*IRIS_std,IRIS_mean+5*IRIS_std,0.001)
plt.plot(IRIS_axis,norm.pdf(IRIS_axis,IRIS_mean,IRIS_std))
plt.show()

"""## **Comparison & Conclusion**

**Holdout＆K-fold**

K-fold為多次計算後結果，因此準確度較為平均；而Holdout在執行多次的結果比較中，會發現準確度浮動值較大。（尤其IRIS資料集中，Holdout的準確度0.89～1.00皆有，但K-fold都在0.95～0.97）

　

**Laplace or not**

在Mushroom的訓練結果中可以發現，經過Laplace的資料，因為降低資料的維度而失真，所以準確率相比於沒經過Laplace的資料準確度較低。
"""

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# jupyter nbconvert --to html /content/ML_HW1.ipynb