# -*- coding: utf-8 -*-
"""ML_HW3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aHooFijB8XuRzNOZ0JZ1C_PjkgYH1I91

# **ML3 - K-means Clustering & Support Vector Machine**

學號：0812203

姓名：蔡銘萱
"""

import pandas as pd
import numpy as np
from sklearn import svm
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, classification_report

"""## **K-means Clustering**

### Data Input
"""

df_feature = ['area', 'perimeter', 'compactness', 'length', 'width', 'coefficient', 'groove', 'label']
df = pd.read_table("./seeds_dataset.txt", sep = '\t*\s', names=df_feature, engine='python')
df

"""### Data Preprocessing"""

df=df.sample(frac=1).reset_index(drop=True)
df

df=df.to_numpy()
df

"""### K-means Clustering"""

def Kmeans(X, num_clusters):

  # random center
  center = []
  minX = X.min(axis=0)
  maxX = X.max(axis=0)
  for i in range(num_clusters):
    team = np.array([])
    for j in range(X.shape[1]):
      index = np.random.randint(minX[j], maxX[j]+1)
      team = np.append(team, index)
    center.append(team)

  cluster = np.array([0]*X.shape[0])

  while True:

    # cluster
    newCluster = np.array([0]*X.shape[0])
    minDis = 999999999
    flag = 0
    for i in range(X.shape[0]):
      for j in range(num_clusters):
        dis = np.sum(np.square(np.subtract(X[i], center[j])))
        if dis < minDis:
          minDis = dis
          flag = j
      newCluster[i] = flag
      minDis = 999999999

    # reseed
    sum = [np.zeros(X.shape[1])]*num_clusters
    num = [0]*num_clusters
    for i in range(X.shape[0]):
      sum[newCluster[i]] = np.add(sum[newCluster[i]], X[i])
      num[newCluster[i]] += 1
    for i in range(num_clusters):
        center[i] = sum[i] / num[i]
    
    # conv
    if (cluster == newCluster).all():
      return cluster, center

    else:
      cluster = newCluster

cluster, center = Kmeans(df, 3)
predict = np.array([0]*df.shape[0])
for i in range(df.shape[0]):
  predict[i] = round(center[cluster[i]][7])
test = np.array([0]*df.shape[0])
for i in range(df.shape[0]):
  test[i] = round(df[i,7])
print("predict:")
print(predict)
print("test:")
print(test)

"""### Result"""

print("\nConfusion matrix: \n",confusion_matrix(test,predict))
print("\nReport: \n",classification_report(test,predict))

"""### Question"""

cluster_Q, center_Q = Kmeans(df[:,1:3], 3)

one_x=[]
one_y=[]
two_x=[]
two_y=[]
thr_x=[]
thr_y=[]
for i in range(df.shape[0]):
  if(cluster_Q[i]==0):
    one_x.append(df[i,1])
    one_y.append(df[i,2])
  elif(cluster_Q[i]==1):
    two_x.append(df[i,1])
    two_y.append(df[i,2])
  else:
    thr_x.append(df[i,1])
    thr_y.append(df[i,2])

plt.scatter(one_x,one_y,c="lightblue")
plt.scatter(two_x,two_y,c="pink")
plt.scatter(thr_x,thr_y,c="navajowhite")
for i in range(3):
  plt.scatter(center_Q[i][0],center_Q[i][1],c="black")

"""## **SVM**

### Data Input
"""

df = pd.read_csv("./ionosphere.data", sep = ',', header = None)
df

"""### Data Preprocessing"""

df=df.sample(frac=1).reset_index(drop=True)
df

df_X = df.drop(columns=[1,34])
df_X

df_y = df[34]
df_y

"""### SVM"""

def linear(X,y):
  X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.7, random_state=0)
  clf = svm.SVC(kernel='linear', C=0.1)
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_valid)
  print("Confusion matrix: \n",confusion_matrix(y_valid,y_pred))
  print("\nReport: \n",classification_report(y_valid,y_pred))
  return

linear(df_X, df_y)

def polynomial(X,y):
  
  kf=KFold(n_splits=5,shuffle=True)
  Q=[]
  Final_degree = 0
  Final_gamma = 0
  Final_coef0 = 0

  for index_train,index_test in kf.split(X):

    x_train=X.iloc[index_train]
    x_test=X.iloc[index_test]
    y_train=y[index_train]
    y_test=y[index_test]

    best_score = 0
    best_degree = 0
    best_gamma = 0
    best_coef0 = 0
    for degree in [2,3,4]:
      for gamma in [0.5,1,1.5]:
        for coef0 in [2,3,4]:
            clf = svm.SVC(kernel='poly', degree=degree, coef0=coef0, gamma=gamma, C=0.1)
            clf.fit(x_train, y_train)
            score = clf.score(x_test,y_test)
            q=[]
            q.append(degree)
            q.append(gamma)
            q.append(coef0)
            q.append(score)
            Q.append(q)
            if score>best_score:
              best_score=score
              best_degree=degree
              best_gamma=gamma
              best_coef0=coef0
    
    Final_degree += best_degree
    Final_gamma += best_gamma
    Final_coef0 += best_coef0

  Final_degree/=5
  Final_gamma/=5
  Final_coef0/=5
  X_train, X_valid, Y_train, Y_valid = train_test_split(X, y, train_size=0.7, random_state=0)
  clf = svm.SVC(kernel='poly', degree=Final_degree, coef0=Final_coef0, gamma=Final_gamma, C=0.1)
  clf.fit(X_train, Y_train)
  y_pred = clf.predict(X_valid)

  print("Best Degree:\t",Final_degree)
  print("Best Coef0:\t",Final_coef0)
  print("Best Gamma:\t",Final_gamma)
  print("\nConfusion matrix: \n",confusion_matrix(Y_valid,y_pred))
  print("\nReport: \n",classification_report(Y_valid,y_pred))

  return Q

ply_Q = polynomial(df_X, df_y)

def RBF(X,y):

  kf=KFold(n_splits=5,shuffle=True)
  Q=[]
  Final_gamma = 0

  for index_train,index_test in kf.split(X):

    x_train=X.iloc[index_train]
    x_test=X.iloc[index_test]
    y_train=y[index_train]
    y_test=y[index_test]

    best_score = 0
    best_gamma = 0
    for gamma in [0.05,0.1,0.15]:
      clf = svm.SVC(kernel='rbf', gamma=gamma, C=0.1)
      clf.fit(x_train, y_train)
      score = clf.score(x_test,y_test)
      q=[]
      q.append(gamma)
      q.append(score)
      Q.append(q)
      if score>best_score:
        best_score=score
        best_gamma=gamma
    Final_gamma+=best_gamma

  Final_gamma/=5
  X_train, X_valid, Y_train, Y_valid = train_test_split(X, y, train_size=0.7, random_state=0)
  clf = svm.SVC(kernel='rbf', gamma=Final_gamma, C=0.1)
  clf.fit(X_train, Y_train)
  y_pred = clf.predict(X_valid)

  print("Best Gamma:\t",Final_gamma)
  print("\nConfusion matrix: \n",confusion_matrix(Y_valid,y_pred))
  print("\nReport: \n",classification_report(Y_valid,y_pred))

  return Q

RBF_Q = RBF(df_X, df_y)

"""### Questions

**polynomial**
"""

col=['degree','gamma','coef0','score']
plt.figure(figsize=(10,1.5))
tab=plt.table(cellText=ply_Q,colLabels=col,loc='center',cellLoc='center')
tab.scale(1,2)
plt.axis('off')

"""**RBF**"""

col=['gamma','score']
plt.figure(figsize=(8,1.5))
tab=plt.table(cellText=RBF_Q,colLabels=col,loc='center',cellLoc='center')
tab.scale(1,2)
plt.axis('off')

"""## **Comparison & Conclusion**

測試SVM時，大部分情況下準確度皆是RBF>Polynomial>Linear，只有少部分情況下Polynomial會略高於RBF。
"""

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# jupyter nbconvert --to html /content/ML_HW3.ipynb